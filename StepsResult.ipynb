{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a5f77f-6b92-458b-9274-e63af243ce88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/mito/anaconda3/lib/python3.11/site-packages (1.35.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/mito/anaconda3/lib/python3.11/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/mito/anaconda3/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mito/anaconda3/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/mito/anaconda3/lib/python3.11/site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in /Users/mito/anaconda3/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/mito/anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/mito/anaconda3/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/mito/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/mito/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mito/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mito/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mito/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/mito/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Requirement already satisfied: pandas in /Users/mito/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /Users/mito/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mito/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mito/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mito/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mito/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install openai\n",
    "! pip install pandas\n",
    "!pip install rouge --quiet\n",
    "!pip install bert_score --quiet\n",
    "!pip install openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1664cc66-a7d9-4840-a4e9-d6a82c3cd899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import openai\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<API KEY>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f31bb0-94ef-4aec-872c-35e58697b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractionAgent:\n",
    "    def __init__(self, csv_file, openai_api_key):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.client = openai.OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    def extract_facts(self, text):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": (\n",
    "                    f\"Please role-play as a medical expert, and extract the true facts from the original dialogue, \"\n",
    "                    f\"reference summary, and the output facts from the following text:\\n\\n\"\n",
    "                    f\"{text}\\n\\n\"\n",
    "                    f\"Facts: A fact is defined as information that cannot be written in more than one sentence. \"\n",
    "                    f\"For instance, the sentence 'The father died of stroke at age 89.' contains three facts: \"\n",
    "                    f\"'The father died.', 'He was 89 years old.', and 'Stroke was the cause of death.' \"\n",
    "                    f\"Ensure that each fact is clearly identified and accurately recorded for comparison and evaluation.\"\n",
    "                )}\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0\n",
    "        )\n",
    "        facts = response.choices[0].message.content.strip().split('\\n')\n",
    "        return [fact.strip() for fact in facts if fact.strip()]\n",
    "    \n",
    "    def extract_all_facts(self):\n",
    "        self.df['dialogue_facts'] = self.df['dialogue'].apply(lambda x: self.extract_facts(x))\n",
    "        self.df['reference_facts'] = self.df['section_text'].apply(lambda x: self.extract_facts(x))\n",
    "        self.df['modelA_facts'] = self.df['Model_A'].apply(lambda x: self.extract_facts(x))\n",
    "        self.df['modelB_facts'] = self.df['Model_B'].apply(lambda x: self.extract_facts(x))\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "387a6ac6-0c5b-4e29-aa6c-9fe76e9b2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'MSc-data/output_summaries.csv'\n",
    "openai_api_key = os.getenv('<API KEY>') \n",
    "extraction_agent = ExtractionAgent(csv_file, openai_api_key)\n",
    "facts_df = extraction_agent.extract_all_facts()\n",
    "facts_df.to_csv('output_facts.csv', index=False)\n",
    "\n",
    "# check fact extraction result\n",
    "for index, row in facts_df.iterrows():\n",
    "    print(f\"Index: {index}\")\n",
    "    print(f\"Dialogue Facts:\")\n",
    "    print(row['dialogue_facts'])\n",
    "    print(f\"Reference Facts:\")\n",
    "    print(row['reference_facts'])\n",
    "    print(f\"Model A Facts:\")\n",
    "    print(row['modelA_facts'])\n",
    "    print(f\"Model B Facts:\")\n",
    "    print(row['modelB_facts'])\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40c11aa-a0ef-42b4-8af4-63d70af9c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotationAgent:\n",
    "    def __init__(self, facts_df, openai_api_key):\n",
    "        self.df = facts_df\n",
    "        self.client = openai.OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    def compare_facts_semantics(self, fact, text):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": (\n",
    "                    f\"Compare the semantic meaning of the following fact with the provided text:\\n\\n\"\n",
    "                    f\"Fact: {fact}\\n\\n\"\n",
    "                    f\"Text: {text}\\n\\n\"\n",
    "                    f\"Consider the fact to be semantically consistent with the text if they express the same or very similar information, even if the wording is different. \"\n",
    "                    f\"For example, 'The father died of a stroke at age 89.' is semantically consistent with 'The father had a stroke and passed away at the age of 89.'.\\n\\n\"\n",
    "                    f\"Please ensure that the correct facts in model A or B do not exceed the number of reference facts. Compare the facts from model A or B with the reference facts, and mark them as correct if they have the same semantic meaning. Otherwise, mark them as incorrect. Facts present in the reference but missing in the models should be marked as omitted. Do not fabricate data..  \"\n",
    "                    f\"Semantic consistency (yes or no):\"\n",
    "                )}\n",
    "            ],\n",
    "            max_tokens=64,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0\n",
    "        )\n",
    "        semantic_consistency = response.choices[0].message.content.strip().lower()\n",
    "        return 'yes' in semantic_consistency\n",
    "\n",
    "    def calculate_facts(self, row):\n",
    "        correct_facts = {'modelA': 0, 'modelB': 0}\n",
    "        incorrect_facts = {'modelA': 0, 'modelB': 0}\n",
    "        omit_facts = {'modelA': 0, 'modelB': 0}\n",
    "        \n",
    "        reference_text = ' '.join(row['reference_facts'])\n",
    "\n",
    "        \n",
    "        for fact in row['modelA_facts']:\n",
    "            if self.compare_facts_semantics(fact, reference_text):\n",
    "                correct_facts['modelA'] += 1\n",
    "            else:\n",
    "                incorrect_facts['modelA'] += 1\n",
    "        \n",
    "        for fact in row['modelB_facts']:\n",
    "            if self.compare_facts_semantics(fact, reference_text):\n",
    "                correct_facts['modelB'] += 1\n",
    "            else:\n",
    "                incorrect_facts['modelB'] += 1        \n",
    "        \n",
    "        correct_facts['modelA'] = min(correct_facts['modelA'], len(row['reference_facts']))\n",
    "        correct_facts['modelB'] = min(correct_facts['modelB'], len(row['reference_facts']))\n",
    "        omit_facts['modelA'] = len(row['reference_facts']) - correct_facts['modelA']\n",
    "        omit_facts['modelB'] = len(row['reference_facts']) - correct_facts['modelB']\n",
    "        \n",
    "        return pd.Series([\n",
    "            len(row['dialogue_facts']),\n",
    "            len(row['reference_facts']),\n",
    "            len(row['modelA_facts']), correct_facts['modelA'], incorrect_facts['modelA'], omit_facts['modelA'],\n",
    "            len(row['modelB_facts']), correct_facts['modelB'], incorrect_facts['modelB'], omit_facts['modelB']\n",
    "        ])\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.df[['num_dialogue_facts', 'num_reference_facts', \n",
    "                 'num_modelA_facts', 'num_modelA_correct_facts', 'num_modelA_incorrect_facts', 'num_modelA_omit_facts',\n",
    "                 'num_modelB_facts', 'num_modelB_correct_facts', 'num_modelB_incorrect_facts', 'num_modelB_omit_facts']] = self.df.apply(self.calculate_facts, axis=1)\n",
    "        \n",
    "        return self.df[['num_dialogue_facts', 'num_reference_facts', \n",
    "                        'num_modelA_facts', 'num_modelA_correct_facts', 'num_modelA_incorrect_facts', 'num_modelA_omit_facts',\n",
    "                        'num_modelB_facts', 'num_modelB_correct_facts', 'num_modelB_incorrect_facts', 'num_modelB_omit_facts']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e47e47e6-32f4-48f3-9ce9-8c4800ff4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_agent = AnnotationAgent(facts_df, openai_api_key)\n",
    "annotation_results = annotation_agent.evaluate()\n",
    "annotation_results.to_csv('output_annotation.csv', index=False)\n",
    "print(annotation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2204d08b-c0ba-4d6a-98d5-6bce248cf156",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You will be given three summaries written for the same article. Your task is to rank these summaries based on their fluency.\n",
    "Please make sure you read and understand these instructions very carefully. \n",
    "Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "{criteria}\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "{steps}\n",
    "\n",
    "Example:\n",
    "\n",
    "Source Text:\n",
    "\n",
    "{document}\n",
    "\n",
    "Summaries:\n",
    "\n",
    "1. {summary1}\n",
    "2. {summary2}\n",
    "3. {summary3}\n",
    "\n",
    "Evaluation Form:\n",
    "\n",
    "- {metric_name}\n",
    "Rank the summaries based on {metric_name}: 1, 2, 3. Please do not give me back any text.\n",
    "Note: Please ensure that the response text only contains rankings in the expected forma, such as 1,2,3. \n",
    "\"\"\"\n",
    "\n",
    "FLUENCY_SCORE_CRITERIA = \"\"\"\n",
    "Fluency evaluates the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\\\n",
    "A good summary has few or no errors and is easy to read and follow.\\\n",
    "Please only return the number from (1-3) as the rank result.\n",
    "\"\"\"\n",
    "\n",
    "FLUENCY_SCORE_STEPS = \"\"\"\n",
    "Please rank these summaries from best to worst based on their fluency. \\\n",
    "Provide the ranks as a list of integers separated by commas (e.g., \"1, 2, 3\").\n",
    "\"\"\"\n",
    "\n",
    "CONCISENESS_SCORE_CRITERIA = \"\"\"\n",
    "Conciseness evaluate the summary should include only important information from the source document. \\\n",
    "The summary should include only important information from the source document. \\\n",
    "The summaries should not contain redundancies and excess information.\\\n",
    "Please only return the number from (1-3) as the rank result.\n",
    "\"\"\"\n",
    "\n",
    "CONCISENESS_SCORE_STEPS = \"\"\"\n",
    "1. Read the summary and the source dialogue carefully.\n",
    "2. Compare the summary to the source dialogue and identify the main points of the article.\n",
    "3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\n",
    "4. Please rank these summaries from best to worst based on their fluency. Provide the ranks as a list of integers separated by commas (e.g., \"1, 2, 3\").\n",
    "\"\"\"\n",
    "   \n",
    "class EvaluationAgent:\n",
    "    def __init__(self, csv_file, annotation_results, openai_api_key):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.annotation_results = annotation_results\n",
    "        self.client = openai.OpenAI(api_key=openai_api_key)  \n",
    "\n",
    "\n",
    "    def get_geval_score(self, criteria: str, steps: str, document: str, summary1: str, summary2: str, summary3: str, metric_name: str):\n",
    "        prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "            criteria=criteria,\n",
    "            steps=steps,\n",
    "            metric_name=metric_name,\n",
    "            document=document,\n",
    "            summary1=summary1,\n",
    "            summary2=summary2,\n",
    "            summary3=summary3,\n",
    "        )\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=256,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "        )\n",
    "        ranks = response.choices[0].message.content.strip()\n",
    "        return [int(rank) for rank in ranks.split(\",\")]\n",
    "\n",
    "        \n",
    "    def compare_matrix(self):\n",
    "        results = []\n",
    "        \n",
    "        for index, row in self.df.iterrows():\n",
    "            document = row['dialogue']\n",
    "            ref_summary = row['section_text']\n",
    "            gen_summaryA = row['Model_A']\n",
    "            gen_summaryB = row['Model_B']\n",
    "            \n",
    "            for eval_type, (criteria, steps) in evaluation_metrics.items():\n",
    "                ranks = self.get_geval_score(criteria, steps, document, ref_summary, gen_summaryA, gen_summaryB, eval_type)\n",
    "                results.append({\n",
    "                    'item_index': index,\n",
    "                    'Evaluation Type': eval_type,\n",
    "                    'Reference_rank': ranks[0],\n",
    "                    'ModelA_rank': ranks[1],\n",
    "                    'ModelB_rank': ranks[2]\n",
    "                })\n",
    "        \n",
    "            num_correct_facts_A = self.annotation_results['num_modelA_correct_facts'][index]\n",
    "            num_total_facts_A = self.annotation_results['num_modelA_facts'][index]\n",
    "            num_correct_facts_B = self.annotation_results['num_modelB_correct_facts'][index]\n",
    "            num_total_facts_B = self.annotation_results['num_modelB_facts'][index]\n",
    "            \n",
    "            consistency_A = num_correct_facts_A / num_total_facts_A if num_total_facts_A > 0 else 0\n",
    "            consistency_B = num_correct_facts_B / num_total_facts_B if num_total_facts_B > 0 else 0\n",
    "            \n",
    "            if consistency_A > consistency_B:\n",
    "                consistency_ranks = [2, 3]\n",
    "            elif consistency_A < consistency_B:\n",
    "                consistency_ranks = [3, 2]\n",
    "            else:\n",
    "                consistency_ranks = [2, 2]\n",
    "            \n",
    "            results.append({\n",
    "                'item_index': index,\n",
    "                'Evaluation Type': 'Consistency',\n",
    "                'Reference_rank': 1,\n",
    "                'ModelA_rank': consistency_ranks[0],\n",
    "                'ModelB_rank': consistency_ranks[1]\n",
    "            })\n",
    "            \n",
    "            num_omit_facts_A = self.annotation_results['num_modelA_omit_facts'][index]\n",
    "            num_dialogue_facts = self.annotation_results['num_dialogue_facts'][index]\n",
    "            omission_rate_A = 1 - (num_omit_facts_A / num_dialogue_facts)\n",
    "            num_omit_facts_B = self.annotation_results['num_modelB_omit_facts'][index]\n",
    "            omission_rate_B = 1 - (num_omit_facts_B / num_dialogue_facts)\n",
    "            \n",
    "            if omission_rate_A > omission_rate_B:\n",
    "                comprehensive_ranks = [2, 3]\n",
    "            elif omission_rate_A < omission_rate_B:\n",
    "                comprehensive_ranks = [3, 2]\n",
    "            else:\n",
    "                comprehensive_ranks = [2, 2]\n",
    "            \n",
    "            results.append({\n",
    "                'item_index': index,\n",
    "                'Evaluation Type': 'Comprehensive',\n",
    "                'Reference_rank': 1,\n",
    "                'ModelA_rank': comprehensive_ranks[0],\n",
    "                'ModelB_rank': comprehensive_ranks[1]\n",
    "            })\n",
    "            \n",
    "        return results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "76d8a327-cdc3-4ec2-b4bd-16f39042a82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item Index: 0, Evaluation Type: Fluency, Reference Rank: 2, ModelA Rank: 3, ModelB Rank: 1\n",
      "Item Index: 0, Evaluation Type: Conciseness, Reference Rank: 2, ModelA Rank: 3, ModelB Rank: 1\n",
      "Item Index: 0, Evaluation Type: Consistency, Reference Rank: 1, ModelA Rank: 3, ModelB Rank: 2\n",
      "Item Index: 0, Evaluation Type: Comprehensive, Reference Rank: 1, ModelA Rank: 2, ModelB Rank: 3\n",
      "Item Index: 1, Evaluation Type: Fluency, Reference Rank: 2, ModelA Rank: 3, ModelB Rank: 1\n",
      "Item Index: 1, Evaluation Type: Conciseness, Reference Rank: 3, ModelA Rank: 2, ModelB Rank: 1\n",
      "Item Index: 1, Evaluation Type: Consistency, Reference Rank: 1, ModelA Rank: 3, ModelB Rank: 2\n",
      "Item Index: 1, Evaluation Type: Comprehensive, Reference Rank: 1, ModelA Rank: 3, ModelB Rank: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_66eba_row0_col0, #T_66eba_row0_col2, #T_66eba_row0_col9, #T_66eba_row0_col11, #T_66eba_row1_col0, #T_66eba_row1_col2, #T_66eba_row1_col9, #T_66eba_row1_col11 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_66eba\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_66eba_level0_col0\" class=\"col_heading level0 col0\" colspan=\"4\">Reference_rank</th>\n",
       "      <th id=\"T_66eba_level0_col4\" class=\"col_heading level0 col4\" colspan=\"4\">ModelA_rank</th>\n",
       "      <th id=\"T_66eba_level0_col8\" class=\"col_heading level0 col8\" colspan=\"4\">ModelB_rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level1\" >Evaluation Type</th>\n",
       "      <th id=\"T_66eba_level1_col0\" class=\"col_heading level1 col0\" >Comprehensive</th>\n",
       "      <th id=\"T_66eba_level1_col1\" class=\"col_heading level1 col1\" >Conciseness</th>\n",
       "      <th id=\"T_66eba_level1_col2\" class=\"col_heading level1 col2\" >Consistency</th>\n",
       "      <th id=\"T_66eba_level1_col3\" class=\"col_heading level1 col3\" >Fluency</th>\n",
       "      <th id=\"T_66eba_level1_col4\" class=\"col_heading level1 col4\" >Comprehensive</th>\n",
       "      <th id=\"T_66eba_level1_col5\" class=\"col_heading level1 col5\" >Conciseness</th>\n",
       "      <th id=\"T_66eba_level1_col6\" class=\"col_heading level1 col6\" >Consistency</th>\n",
       "      <th id=\"T_66eba_level1_col7\" class=\"col_heading level1 col7\" >Fluency</th>\n",
       "      <th id=\"T_66eba_level1_col8\" class=\"col_heading level1 col8\" >Comprehensive</th>\n",
       "      <th id=\"T_66eba_level1_col9\" class=\"col_heading level1 col9\" >Conciseness</th>\n",
       "      <th id=\"T_66eba_level1_col10\" class=\"col_heading level1 col10\" >Consistency</th>\n",
       "      <th id=\"T_66eba_level1_col11\" class=\"col_heading level1 col11\" >Fluency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >item_index</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "      <th class=\"blank col11\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_66eba_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_66eba_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_66eba_row0_col1\" class=\"data row0 col1\" >2</td>\n",
       "      <td id=\"T_66eba_row0_col2\" class=\"data row0 col2\" >1</td>\n",
       "      <td id=\"T_66eba_row0_col3\" class=\"data row0 col3\" >2</td>\n",
       "      <td id=\"T_66eba_row0_col4\" class=\"data row0 col4\" >2</td>\n",
       "      <td id=\"T_66eba_row0_col5\" class=\"data row0 col5\" >3</td>\n",
       "      <td id=\"T_66eba_row0_col6\" class=\"data row0 col6\" >3</td>\n",
       "      <td id=\"T_66eba_row0_col7\" class=\"data row0 col7\" >3</td>\n",
       "      <td id=\"T_66eba_row0_col8\" class=\"data row0 col8\" >3</td>\n",
       "      <td id=\"T_66eba_row0_col9\" class=\"data row0 col9\" >1</td>\n",
       "      <td id=\"T_66eba_row0_col10\" class=\"data row0 col10\" >2</td>\n",
       "      <td id=\"T_66eba_row0_col11\" class=\"data row0 col11\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66eba_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_66eba_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_66eba_row1_col1\" class=\"data row1 col1\" >3</td>\n",
       "      <td id=\"T_66eba_row1_col2\" class=\"data row1 col2\" >1</td>\n",
       "      <td id=\"T_66eba_row1_col3\" class=\"data row1 col3\" >2</td>\n",
       "      <td id=\"T_66eba_row1_col4\" class=\"data row1 col4\" >3</td>\n",
       "      <td id=\"T_66eba_row1_col5\" class=\"data row1 col5\" >2</td>\n",
       "      <td id=\"T_66eba_row1_col6\" class=\"data row1 col6\" >3</td>\n",
       "      <td id=\"T_66eba_row1_col7\" class=\"data row1 col7\" >3</td>\n",
       "      <td id=\"T_66eba_row1_col8\" class=\"data row1 col8\" >2</td>\n",
       "      <td id=\"T_66eba_row1_col9\" class=\"data row1 col9\" >1</td>\n",
       "      <td id=\"T_66eba_row1_col10\" class=\"data row1 col10\" >2</td>\n",
       "      <td id=\"T_66eba_row1_col11\" class=\"data row1 col11\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x15b098990>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics = {\n",
    "    \"Fluency\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\n",
    "    \"Conciseness\": (CONCISENESS_SCORE_CRITERIA, CONCISENESS_SCORE_STEPS),\n",
    "}\n",
    "\n",
    "evaluation_agent = EvaluationAgent(csv_file, annotation_results, openai.api_key)\n",
    "sorted_results = evaluation_agent.compare_matrix()\n",
    "\n",
    "for result in sorted_results:\n",
    "    print(f\"Item Index: {result['item_index']}, Evaluation Type: {result['Evaluation Type']}, Reference Rank: {result['Reference_rank']}, ModelA Rank: {result['ModelA_rank']}, ModelB Rank: {result['ModelB_rank']}\")\n",
    "\n",
    "# check rank result\n",
    "df_results = pd.DataFrame(sorted_results)\n",
    "pivot_df = df_results.pivot(index='item_index', columns='Evaluation Type', values=['Reference_rank', 'ModelA_rank', 'ModelB_rank'])\n",
    "styled_pivot_df = pivot_df.style.apply(lambda x: [\"background: yellow\" if v == 1 else \"\" for v in x], axis=1)\n",
    "styled_pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb2df78c-d774-4484-a6c4-37975ac32607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "output_csv_file = 'MSc-data/evaluation_results.csv'\n",
    "df_results.to_csv(output_csv_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
